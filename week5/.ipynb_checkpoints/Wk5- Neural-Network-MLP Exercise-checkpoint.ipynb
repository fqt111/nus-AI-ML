{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks (NN) / Multilayer Perceptrons (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall that machine learning algorithms are nothing but function approximation.  This is true for Neural Networks too.\n",
    "\n",
    "#### Let's start with a very simple neural network algorithm called the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Perceptron** (shown below) is a very simple **linear binary classifier**. It basically maps and input vector x to a binary output f(x). \n",
    "\n",
    "Given a weight vector w, the Perceptron's classfication rule is: f(x) = 1 if w â‹… x + b > 0 or f(x) = 0 otherwise. \n",
    "\n",
    "The training step is used to learn the optimal values of the weights w and b.\n",
    "\n",
    "Here, b is a bias value which is responsible for shifting the Perceptron's hyperplane away from the origin.\n",
    "\n",
    "Question: Does this remind you of something you have recently seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step function at the output is called the *activation function*.\n",
    "\n",
    "The most simple examples for Perceptron are the basic logic operations, such as: AND, OR and XOR. The truth tables for these logic functions is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.talkingelectronics.com/pay/PIC/TruthTable-2.gif\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"http://www.talkingelectronics.com/pay/PIC/TruthTable-2.gif\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical view of these three logic functions is shown below. We see that the AND and OR logic functions are linearly separable but the XOR is not. What do you think this means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Perceptron algorithm to learn these three logical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Setting random seed.\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input samples\n",
    "X = np.array([[0, 0],\n",
    "[0, 1],\n",
    "[1, 0],\n",
    "[1, 1]],\n",
    "dtype=np.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron for AND\n",
      "Expected Output: [0 0 0 1]\n",
      "Predicted Output: [0 0 0 1]\n",
      "Accuracy: 1.00\n",
      "Confidence scores: [-2.00e-01 -2.78e-17 -1.00e-01  1.00e-01]\n"
     ]
    }
   ],
   "source": [
    "# Setting the expected outputs for AND\n",
    "y_AND = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Creating and training a Perceptron.\n",
    "p = Perceptron(random_state=seed, eta0=0.1, max_iter=1000)\n",
    "p.fit(X, y_AND)\n",
    "y_pred = p.predict(X)\n",
    "print('Perceptron for AND')\n",
    "print('Expected Output:',y_AND)\n",
    "print('Predicted Output:',y_pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_AND, y_pred))\n",
    "\n",
    "# Obtaining confidence scores.\n",
    "np.set_printoptions(precision=2)\n",
    "pred_scores = p.decision_function(X)\n",
    "print(\"Confidence scores: {}\".format(pred_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron for OR\n",
      "Expected Output: [0 1 1 1]\n",
      "Predicted Output: [0 1 1 1]\n",
      "Accuracy: 1.00\n",
      "Confidence scores: [-0.1  0.1  0.1  0.3]\n"
     ]
    }
   ],
   "source": [
    "# Setting the expected outputs for OR\n",
    "y_OR = np.array([0, 1, 1, 1])\n",
    "#p.fit(X, y_OR)\n",
    "\n",
    "# Creating and training a Perceptron.\n",
    "p = Perceptron(random_state=seed, eta0=0.1, max_iter=1000)\n",
    "p.fit(X, y_OR)\n",
    "y_pred = p.predict(X)\n",
    "print('Perceptron for OR')\n",
    "print('Expected Output:',y_OR)\n",
    "print('Predicted Output:',y_pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_OR, y_pred))\n",
    "\n",
    "# Obtaining confidence scores.\n",
    "np.set_printoptions(precision=2)\n",
    "pred_scores = p.decision_function(X)\n",
    "print(\"Confidence scores: {}\".format(pred_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron for XOR\n",
      "Expected Output: [0 1 1 0]\n",
      "Predicted Output: [0 0 0 0]\n",
      "Accuracy: 0.50\n",
      "Confidence scores: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Setting the expected outputs for XOR\n",
    "y_XOR = np.array([0, 1, 1, 0])\n",
    "#p.fit(X, y_XOR)\n",
    "\n",
    "# Creating and training a Perceptron.\n",
    "p = Perceptron(random_state=1010, eta0=0.1, max_iter=1000)\n",
    "p.fit(X, y_XOR)\n",
    "y_pred = p.predict(X)\n",
    "print('Perceptron for XOR')\n",
    "print('Expected Output:',y_XOR)\n",
    "print('Predicted Output:',y_pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_XOR, y_pred))\n",
    "\n",
    "# Obtaining confidence scores.\n",
    "np.set_printoptions(precision=2)\n",
    "pred_scores = p.decision_function(X)\n",
    "print(\"Confidence scores: {}\".format(pred_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, XOR is not a linearly separable problem. In other words, it is not possible to separate the two classes with a single hyperplane.\n",
    "\n",
    "This kind of problem motivates us to use Multilayer Perceptrons (MLPs), which we explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP is a neural network which is composed by at least three different layers: an input layer, a hidden layer and an output layer. \n",
    "\n",
    "Except for the input layer, the remaining ones are composed by Perceptrons (we call them nodes) with nonlinear activation functions (e.g., sigmoid or tanh).\n",
    "\n",
    "MLPs are usually trained using the backpropagation algorithm and are able to deal with not linearly separable problems. The training step is used to learn the weights on the edges between the nodes.\n",
    "\n",
    "Below we use the MLP for the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.saedsayad.com/images/Perceptron_bkp_1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image(url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/296px-Colored_neural_network.svg.png\", width=300)\n",
    "\n",
    "Image(url= \"http://www.saedsayad.com/images/Perceptron_bkp_1.png\", width=700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP for XOR\n",
      "Expected Output: [0 1 1 0]\n",
      "Predicted Output: [0 1 1 0]\n",
      "Accuracy: 1.00\n",
      "MLP's XOR probabilities:\n",
      "[class0, class1]\n",
      "[[0.98 0.02]\n",
      " [0.07 0.93]\n",
      " [0.15 0.85]\n",
      " [0.83 0.17]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a MLPClassifier.\n",
    "# hidden_layer_sizes receive a tuple where each position i indicates the number of neurons\n",
    "# in the ith hidden layer\n",
    "# activation specifies the activation function (other options are: 'identity', 'logistic' and 'relu')\n",
    "# max_iter indicates the maximum number of training iterations\n",
    "# There are other parameters which can also be changed.\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(3,),\n",
    "activation='tanh',\n",
    "max_iter=10000,\n",
    "random_state=10101)\n",
    "\n",
    "# Training and plotting the decision boundary.\n",
    "mlp.fit(X, y_XOR)\n",
    "\n",
    "y_pred = mlp.predict(X)\n",
    "print('MLP for XOR')\n",
    "print('Expected Output:',y_XOR)\n",
    "print('Predicted Output:',y_pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_XOR, y_pred))\n",
    "\n",
    "# Obtaining probabiliteis\n",
    "pred = mlp.predict_proba(X)\n",
    "print(\"MLP's XOR probabilities:\\n[class0, class1]\\n{}\".format(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-before: always\">\n",
    "\n",
    "## MLP for Wisconsin Diagnostic Breast Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80e+01, 1.04e+01, 1.23e+02, 1.00e+03, 1.18e-01, 2.78e-01,\n",
       "        3.00e-01, 1.47e-01, 2.42e-01, 7.87e-02, 1.09e+00, 9.05e-01,\n",
       "        8.59e+00, 1.53e+02, 6.40e-03, 4.90e-02, 5.37e-02, 1.59e-02,\n",
       "        3.00e-02, 6.19e-03, 2.54e+01, 1.73e+01, 1.85e+02, 2.02e+03,\n",
       "        1.62e-01, 6.66e-01, 7.12e-01, 2.65e-01, 4.60e-01, 1.19e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading Breast Cancer dataset.\n",
    "wdbc = load_breast_cancer()\n",
    "df = pd.DataFrame(wdbc.data, columns=wdbc.feature_names)\n",
    "wdbc.data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdbc.target[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['malignant', 'benign']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wdbc.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split\\\n",
    "(wdbc.data, wdbc.target, test_size=0.33, stratify=wdbc.target, \\\n",
    " random_state=np.random.randint(1,10))\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),\n",
    "activation='tanh',\n",
    "max_iter=10000,\n",
    "random_state=np.random.randint(1,10))\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP's accuracy score: 0.925531914893617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"MLP's accuracy score: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that its accuracy score is rather low.\n",
    "\n",
    "Unfortunately, MLPs are very sensitive to different feature scales. So, it is normally necessary to normalize or rescale the input data. With data normalization, we see that the accuracy improves considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler. This object normalizes features to zero mean and unit variance.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# Normalizing train and test data.\n",
    "X_train_scaled, X_test_scaled = scaler.transform(X_train), scaler.transform(X_test)\n",
    "# Training MLP with normalized data.\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "# Testing MLP with normalized data.\n",
    "y_pred = mlp.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP's accuracy score: 0.973404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.96      0.97      0.96        70\n",
      "      benign       0.98      0.97      0.98       118\n",
      "\n",
      "    accuracy                           0.97       188\n",
      "   macro avg       0.97      0.97      0.97       188\n",
      "weighted avg       0.97      0.97      0.97       188\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 68,   2],\n",
       "       [  3, 115]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"MLP's accuracy score: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred, target_names=wdbc.target_names))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
